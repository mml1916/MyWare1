{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练算法：使用梯度上升找到最佳参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取textSet.txt并处理\n",
    "def loadDataSet():\n",
    "    dataMat = []; labelMat = []\n",
    "    fr = open('testSet.txt')\n",
    "    for line in fr.readlines():\n",
    "        #strip移除字符串头尾指定的字符（默认为空格），返回移除字符串头尾指定的字符生成的新字符串。\n",
    "        # split()通过指定分隔符对字符串进行切片，返回分割后的字符串列表。\n",
    "        lineArr = line.strip().split()\n",
    "        #向数组添加列表，每个列表包含1.0和两个特征值\n",
    "        dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])\n",
    "        labelMat.append(int(lineArr[2]))\n",
    "    return dataMat,labelMat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoid函数\n",
    "def sigmoid(inX):\n",
    "    return 1.0/(1+exp(-inX))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#梯度上升算法\n",
    "def gradAscent(dataMatIn, classLabels):\n",
    "    dataMatrix = mat(dataMatIn)             #numpy样本 数组（每个样本包含2个特征和第0维特征，共100个样本） convert to NumPy matrix\n",
    "    labelMat = mat(classLabels).transpose() #类别标签，它是一个1×100的行向量，convert to NumPy matrix，并转置\n",
    "    m,n = shape(dataMatrix)\n",
    "    #向目标移动的步长\n",
    "    alpha = 0.001\n",
    "    #迭代次数\n",
    "    maxCycles = 500\n",
    "    #创建n×1的全1向量\n",
    "    weights = ones((n,1))\n",
    "    #循环迭代完成之后，返回训练好的回归系数\n",
    "    for k in range(maxCycles):              #heavy on matrix operations\n",
    "        h = sigmoid(dataMatrix*weights)     #矩阵乘法的结果作为sigmoid函数的参数\n",
    "        error = (labelMat - h)              #vector subtraction 计算真实类别与预测类别的差值\n",
    "        #更新回归系数\n",
    "        weights = weights + alpha * dataMatrix.transpose()* error #matrix mult\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析数据：画出决策边界 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#画出数据集和Logistic回归最佳拟合直线的函数\n",
    "def plotBestFit(weights):\n",
    "    import matplotlib.pyplot as plt\n",
    "    #获取数据\n",
    "    dataMat,labelMat=loadDataSet()\n",
    "    #转换成数组\n",
    "    dataArr = array(dataMat)\n",
    "    #获得样本容量\n",
    "    n = shape(dataArr)[0] \n",
    "    xcord1 = []; ycord1 = []\n",
    "    xcord2 = []; ycord2 = []\n",
    "    #循环遍历\n",
    "    for i in range(n):\n",
    "        #若类别标签为1，将其对应样本的第1维和第2维特征值分 别添加进xcord1和ycord1列表\n",
    "        if int(labelMat[i])== 1:\n",
    "            xcord1.append(dataArr[i,1]); ycord1.append(dataArr[i,2])\n",
    "        #类别标签不为1，同上\n",
    "        else:\n",
    "            xcord2.append(dataArr[i,1]); ycord2.append(dataArr[i,2])\n",
    "    #创建画布\n",
    "    fig = plt.figure()\n",
    "    #创建子图\n",
    "    ax = fig.add_subplot(111)\n",
    "    #散列图\n",
    "    ax.scatter(xcord1, ycord1, s=30, c='red', marker='s')\n",
    "    ax.scatter(xcord2, ycord2, s=30, c='green')\n",
    "    #生成数组,源代码没有reshape（1，60），运行会报错，提示x和y维度不同，所以进行了修改，添加了reshape\n",
    "    x = arange(-3.0, 3.0, 0.1).reshape(1,60)\n",
    "    #sigmoid函数为0.5（0 = z = w0x0+w1x1+w2x2）时，是两个分类（类别1和类别0 ) 的分界处,在loadDataSet函数中，x0也即第0维特征被设置为了1，\n",
    "    #，x1就是上一行的x，y = x2 = （-w0-w1x1）/w2 = （-w0-w1x）/w2 ，即下式。\n",
    "    y = (-weights[0]-weights[1]*x)/weights[2]\n",
    "    #画线\n",
    "    ax.plot(x, y)\n",
    "    #横轴和纵轴标签\n",
    "    plt.xlabel('X1'); plt.ylabel('X2');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练算法改进：随机梯度上升 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度上升算法在每次更新回归系数时都需要遍历整个数据集，该方法在处理数据量较大时计算复杂度就太高了，\n",
    "一种改进方法是一次仅用一个样本点来更新回归系数，该方法称为随机梯度上升算法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#随机梯度上升法版本1\n",
    "#随机梯度上升算法与梯度上升算法在代码上很相似，但也有一些区别：第一，后\n",
    "#者的变量h和误差error都是向量，而前者则全是数值；第 二 ，前者没有矩阵的转换过程，所有\n",
    "#变量的数据类型都是N UmPy数组。\n",
    "def stocGradAscent0(dataMatrix, classLabels):\n",
    "    m,n = shape(dataMatrix)\n",
    "    alpha = 0.01\n",
    "    weights = ones(n)   #initialize to all ones\n",
    "    for i in range(m):\n",
    "        #print dataMatrix[i]\n",
    "        #因为是数组，数组乘法和矩阵不一样，所以要用到sum函数\n",
    "        h = sigmoid(sum(dataMatrix[i]*weights))\n",
    "       # print h\n",
    "        error = classLabels[i] - h\n",
    "       # print error\n",
    "        #更新回归系数\n",
    "        weights = weights + alpha * error * dataMatrix[i]\n",
    "        print (alpha * error * dataMatrix[i])\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataArr,labels = loadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#随机梯度上升法版本2\n",
    "def stocGradAscent1(dataMatrix, classLabels, numIter=150):\n",
    "    m,n = shape(dataMatrix)\n",
    "    weights = ones(n)   #initialize to all ones\n",
    "    #循环遍历，进算法还增加了一个迭代次数作为第3个参数。如果该参数没有给定的话，算法将\n",
    "    #默认迭代150次。如果给定，那么算法将按照新的参数值进行迭代。\n",
    "    for j in range(numIter):\n",
    "        dataIndex = range(m)\n",
    "        for i in range(m):\n",
    "            #alpha在每次迭代的时候都会调整，缓解数据波动或者高频波动\n",
    "            #虽然alpha会随着迭代次数不断减小，但永远不会减小到0，这是因为下式中\n",
    "            #还存在一个常数项^ 必须这样做的原因是为了保证在多次迭代之后新数据仍然具有一定的影响\n",
    "            alpha = 4/(1.0+j+i)+0.0001    #apha decreases with iteration, does not \n",
    "            #通过随机选取样本来更新回归系数。这种方法将减少周期性的波动\n",
    "            randIndex = int(random.uniform(0,len(dataIndex)))#go to 0 because of the constant\n",
    "            h = sigmoid(sum(dataMatrix[randIndex]*weights))\n",
    "            error = classLabels[randIndex] - h\n",
    "            weights = weights + alpha * error * dataMatrix[randIndex]\n",
    "            #随机从列表中选出一个值，然后从列表中删掉该值（再进行下一次迭代)\n",
    "            del(dataIndex[randIndex])\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataArr,labels = loadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wei = stocGradAscent0(dataArr,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 项目示例：从疝气病症预测病马的死亡率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了部分指标主观和难以测量外，该数据还存在一个问题，数据集中有30%的值是缺失的\n",
    "\n",
    "处理被据中的缺失值:\n",
    "\n",
    "□使用可用特征的均值来填补缺失值；\n",
    "\n",
    "□使用特殊值来±真补缺失值，如-1;\n",
    "\n",
    "□ 忽略有缺失值的样本；\n",
    "\n",
    "□使用相似样本的均值添补缺失值；\n",
    "\n",
    "□使用另外的机器学习算法预测缺失值。\n",
    "本例中的处理方法：1、对于缺失的特征值选择实数0来替换 2、一条数据的类别标签已经缺失，那么我们的简单做法是将该条数据丢弃。\n",
    "原始的数据集经过预处理之后保存成两个文件：horseColicTest. txt和 horseColicTraining.txt\n",
    "\n",
    "\n",
    "开发流程\n",
    "\n",
    "收集数据: 给定数据文件 \n",
    "\n",
    "准备数据: 用 Python 解析文本文件并填充缺失值\n",
    "\n",
    "分析数据: 可视化并观察数据\n",
    "\n",
    "训练算法: 使用优化算法，找到最佳的系数\n",
    "\n",
    "测试算法: 为了量化回归的效果，需要观察错误率。根据错误率决定是否回退到训练阶段，\n",
    "         通过改变迭代的次数和步长的参数来得到更好的回归系数\n",
    "         \n",
    "使用算法: 实现一个简单的命令行程序来手机马的症状并输出预测结果并非难事，\n",
    "         这可以作为留给大家的一道习题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#回归分类函数 以回归系数和特征向量作为输入来计算对应的sigmoid值。如果sigmoid值大于0.5函数返回1，否则返回0。\n",
    "def classifyVector(inX, weights):\n",
    "    prob = sigmoid(sum(inX*weights))\n",
    "    if prob > 0.5: return 1.0\n",
    "    else: return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#函数colicTest()，是用于打开测试集和训练集，并对数据进行格式化处理的函数\n",
    "#数据的最后一列仍然是类别标签。数据最初有三个类别标签，分别代表马的三种情况：“仍存活”、“已经死亡”和 “已经安乐死”。这里为了方便，\n",
    "#将 “已经死亡”和 “已经安乐死”合 并 成 “未能存活”这 个 标 签\n",
    "def colicTest():\n",
    "    #打开训练及和测试集\n",
    "    frTrain = open('horseColicTraining.txt'); frTest = open('horseColicTest.txt')\n",
    "    #创建列表\n",
    "    trainingSet = []; trainingLabels = []\n",
    "    #循环遍历\n",
    "    for line in frTrain.readlines():\n",
    "        #去除空白符并以‘\\t’进行分割\n",
    "        currLine = line.strip().split('\\t')\n",
    "        lineArr =[]\n",
    "        #将每一个样本的前21个数值一次添加进lineArr列表\n",
    "        for i in range(21):\n",
    "            lineArr.append(float(currLine[i]))\n",
    "        trainingSet.append(lineArr)\n",
    "        #将下表为21的数值（类别标签）添加进trainingLabels\n",
    "        trainingLabels.append(float(currLine[21]))\n",
    "    #利用随机梯度上升法进行训练，返回训练后得到的回归系数\n",
    "    trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, 1000)\n",
    "    errorCount = 0; numTestVec = 0.0\n",
    "    #循环遍历测试数据集\n",
    "    for line in frTest.readlines():\n",
    "        numTestVec += 1.0\n",
    "        currLine = line.strip().split('\\t')\n",
    "        lineArr =[]\n",
    "        for i in range(21):\n",
    "            lineArr.append(float(currLine[i]))\n",
    "        #进行分类，并对分类结果进行检查\n",
    "        if int(classifyVector(array(lineArr), trainWeights))!= int(currLine[21]):\n",
    "            errorCount += 1\n",
    "    #计算错误率\n",
    "    errorRate = (float(errorCount)/numTestVec)\n",
    "    print \"the error rate of this test is: %f\" % errorRate\n",
    "    #返回错误率\n",
    "    return errorRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiTeSt ()，其功能是调用函数 colicTest()10次并求结果的平均值\n",
    "def multiTest():\n",
    "    numTests = 10; errorSum=0.0\n",
    "    for k in range(numTests):\n",
    "        errorSum += colicTest()\n",
    "    print \"after %d iterations the average error rate is: %f\" % (numTests, errorSum/float(numTests))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate of this test is: 0.328358\n",
      "the error rate of this test is: 0.298507\n",
      "the error rate of this test is: 0.343284\n",
      "the error rate of this test is: 0.328358\n",
      "the error rate of this test is: 0.402985\n",
      "the error rate of this test is: 0.343284\n",
      "the error rate of this test is: 0.417910\n",
      "the error rate of this test is: 0.283582\n",
      "the error rate of this test is: 0.432836\n",
      "the error rate of this test is: 0.238806\n",
      "after 10 iterations the average error rate is: 0.341791\n"
     ]
    }
   ],
   "source": [
    "multiTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
