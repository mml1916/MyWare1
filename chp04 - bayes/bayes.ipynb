{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优点：在数据较少的情况下仍然有效，可以处理多类别问题 \n",
    "### 缺点：对于输入数据的准备方式较为敏感。\n",
    "### 适用数据类型：标称型数据\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  朴素贝叶斯一般过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.收集数据：可以使用任何方法\n",
    "#### 2.准备数据：需要数值型或者布尔型数据\n",
    "#### 3.有大量特征时，绘制特征作用不大，此时使用直方图效果更好\n",
    "#### 4.训练算法：计算不同独立特征的条件概率\n",
    "#### 5.测试算法：计算错误率\n",
    "#### 6.使用算法：一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  一 准备数据：从文本中构建词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建实验样本 该函数返回的第一个变量是进行词条切分后的文档集合\n",
    "#返回的第二个变量是一个类别标签的集合。这里有两类，侮辱性和非侮辱性\n",
    "def loadDataSet():\n",
    "    postingList = [['my','dog','has','flea','problem','help','please'],\\\n",
    "                   ['maybe','not','take','him','to','dog','park','stupid'],\\\n",
    "                   ['my','dalmation','is','so','cute','I','love','him'],\\\n",
    "                   ['stop','posting','stupid','worthless','garbage'],\\\n",
    "                   ['mr','licks','ate','my','steak','how','to','stop','him'],\\\n",
    "                   ['quit','buying','worthless','dog','food','stupid']]\n",
    "    classVec = [0,1,0,1,0,1] #1代表侮辱性文字，0代表正常言论\n",
    "    return postingList,classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建一个包含在所有文档中出现的不重复词的列表\n",
    "def createVocabList(dataSet):\n",
    "    #set() 函数创建一个无序不重复元素集，可进行关系测试，删除重复数据，还可以计算交集、差集、并集等\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        #两个set集合求并集\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    #生成不重复的词汇列表\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#函数的输人参数为词汇表及某个文档 ，输出的是文档向量，向量的每一元素为1或0 ，分别表示词汇表中的单词在输人文档中是否出现\n",
    "def setOfWords2Vec(vocabList,inputSet):\n",
    "    #列表对 + 和 * 的操作符与字符串相似。+ 号用于组合列表，* 号用于重复列表。\n",
    "    returnVec = [0]*len(vocabList)#初始化一个和词汇表相同大小的向量;\n",
    "    for word in inputSet:#遍历文档中的单词\n",
    "        if word in vocabList:#如果出现了词汇表中的单词，则将输出的文档向量（returnVec）中的对应值设为1\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:print \"the word:%s is not in my Vocabulary!\" %word#否则打印不在列表中\n",
    "    #返回文档向量\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listOPosts, listClasses = loadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myVocabList = createVocabList(listOPosts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二 训练算法：从词向量计算概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入参数是文档矩阵trainMatrix，以及每篇文档类别标签所构成的向量。\n",
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    #获取在测试文档矩阵中有几篇文档\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    #获取第一篇文档的单词长度\n",
    "    numWords = len(trainMatrix[0])\n",
    "    #类别为1的个数除以总篇数，就得到类别为1的文档在总文档数中所占的比例\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num = ones(numWords);p1Num = ones(numWords)#初始化求概率的分子变量和分母变量，\n",
    "    p0Denom = 2.0;p1Denom = 2.0  #这里防止有一个p(xn|1)为0，则最后的乘积也为0，所有将分子初始化为1，分母初始化为2。\n",
    "    #p0Num = zeros(numWords);p1Num = zeros(numWords)\n",
    "    #p0Denom = 0.0;p1Denom = 0.0\n",
    "    for i in range(numTrainDocs):#遍历每一篇训练文档\n",
    "        if trainCategory[i] == 1:#如果这篇文档的类别是1\n",
    "            # 对所有属于类别1的文档向量按位置累加，\n",
    "            #最终形成的向量其实表示了在类别1下，一共包含了多少个词汇表中的不同单词（指列，一列表示一个单词，每一列都不重复）\n",
    "            #每一列的值表示该列的单词在属于类别1的所有文档中出现了几次\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])#sum函数计算属于类别1对应文档向量的和,同时进行累加，实质是求属于类别1的单词个数之和\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]#\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    #p1Vect = p1Num / p1Denom #属于类别1的单词在属于类别1的所有单词的概率，是一个向量，每一个元素都是一个单词的概率\n",
    "    #p0Vect =p0Num/p0Denom #类别0的，同上，\n",
    "    p1Vect = log(p1Num / p1Denom)#修改为取对数防止程序下溢出或者得不到正确答案\n",
    "    p0Vect = log(p0Num/p0Denom)\n",
    "    return p0Vect,p1Vect,pAbusive #返回两个类别的概率向量与一个属于侮辱性文档的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOPosts, listClasses = loadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "myVocabList = createVocabList(listOPosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMat = []#创建一个列表\n",
    "for postinDoc in listOPosts:\n",
    "    #for循环结束后，trainMat变为一个矩阵（按矩阵来理解），该矩阵的列数与词汇表myVocabList相等，行数与listOPosts相等，\n",
    "    trainMat.append(setOfWords2Vec(myVocabList,postinDoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0V,p1V,pAb = trainNB0(trainMat, listClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用算法：\n",
    "        # 将乘法转换为加法\n",
    "        # 乘法：P(C|F1F2...Fn) = P(F1F2...Fn|C)P(C)/P(F1F2...Fn)\n",
    "        # 加法：P(F1|C)*P(F2|C)....P(Fn|C)P(C) -> log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n",
    "        # :param vec2Classify: 待测数据[0,1,1,1,1...]，即要分类的向量\n",
    "        # :param p0Vec: 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表\n",
    "        #:param p1Vec: 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表\n",
    "        #:param pClass1: 类别1，侮辱性文件的出现概率\n",
    " # 计算公式  log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n",
    " #上面的计算公式，没有除以贝叶斯准则的公式的分母，也就是 P(w)，就进行概率大小的比较了。P(w) 指的是此文档在所有的文档中出现的概率\n",
    " #（此文档就是指该待测向量，在此例中一个文档就是一条留言板留言）。在此先理解为对于类别1和类别0，p（w）值一样\n",
    " # 因为 P(w) 针对的是包含侮辱和非侮辱的全部文档，所以 P(w) 是相同的。\n",
    "# P(F1F2...Fn|C)P(C) = P(F1|C)*P(F2|C)....P(Fn|C)P(C) =log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))该等式在下面求p1、p0时，\n",
    "#从右往左来理解\n",
    "## 我的理解是：这里的 vec2Classify * p1Vec 的意思就是求vec2Classify中每个词在类别1或类别2中出现的概率，vec2Classify也是由0、1组成的向量\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):  \n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)  \n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1-pClass1)  \n",
    "    if p1 > p0:  \n",
    "        return 1  \n",
    "    else:  \n",
    "        return 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingNB():\n",
    "    \"\"\"\n",
    "    测试朴素贝叶斯算法\n",
    "    \"\"\"\n",
    "    # 1. 加载数据集\n",
    "    listOPosts, listClasses = loadDataSet()\n",
    "    # 2. 创建单词集合\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    # 3. 创建数据矩阵，\n",
    "    trainMat = []\n",
    "    for postinDoc in listOPosts:\n",
    "        # 返回m*len(myVocabList)的矩阵， 记录的都是0，1信息\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    # 4. 训练数据\n",
    "    p0V, p1V, pAb = trainNB0(array(trainMat), array(listClasses))\n",
    "    # 5. 测试数据\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb)\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 项目案例2: 使用朴素贝叶斯过滤垃圾邮件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "开发流程\n",
    "\n",
    " 使用朴素贝叶斯对电子邮件进行分类\n",
    "\n",
    "收集数据: 提供文本文件\n",
    "准备数据: 将文本文件解析成词条向量\n",
    "分析数据: 检查词条确保解析的正确性\n",
    "训练算法: 使用我们之前建立的 trainNB0() 函数\n",
    "测试算法: 使用clasSifyNB(），并且构建一个新的测试函数来计算文档集的错误率。\n",
    "使用算法: 构建一个完整的程序对一组文档进行分类，将错分的文档输出到屏幕上\n",
    "'''\n",
    "\n",
    "#准备数据: 将文本文件解析成词条向量\n",
    "\n",
    "\n",
    "#切分文本\n",
    "def textParse(bigString):\n",
    "    '''\n",
    "    Desc:接收一个大字符串并将其解析为字符串列表\n",
    "    Args: bigString -- 大字符串\n",
    "    Returns:去掉少于 2 个字符的字符串，并将所有字符串转换为小写，返回字符串列表\n",
    "    '''\n",
    "    import re\n",
    "    # 使用正则表达式来切分句子，其中分隔符是除单词、数字、下划线外的任意字符串\n",
    "    listOfTokens = re.split(r'\\W*', bigString)\n",
    "    #去除少于2个字符的字符串（if len（tok）> 0），将字符串全部转换成小写（.lower()或者大写.upper()）\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]\n",
    "\n",
    "def spamTest():\n",
    "    '''\n",
    "    Desc:\n",
    "        对贝叶斯垃圾邮件分类器进行自动化处理。\n",
    "    Args:\n",
    "        none\n",
    "    Returns:\n",
    "        对测试集中的每封邮件进行分类，若邮件分类错误，则错误数加 1，最后返回总的错误百分比。\n",
    "    '''\n",
    "    docList = []\n",
    "    classList = []\n",
    "    fullText = []\n",
    "    for i in range(1, 26):\n",
    "        # 切分，解析数据，并归类为 1 类别\n",
    "        wordList = textParse(open('./email/spam/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        # 切分，解析数据，并归类为 0 类别\n",
    "        wordList = textParse(open('./email/ham/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "     # 创建词汇表\n",
    "    vocabList = createVocabList(docList)\n",
    "    trainingSet = range(50)\n",
    "    \n",
    "    testSet = []\n",
    "    # 随机取 10 个邮件用来测试\n",
    "    for i in range(10):\n",
    "        # random.uniform(x, y) 随机生成一个范围在 [x,y) 之间的实数\n",
    "        randIndex = int(random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])\n",
    "    #随机取10个后，剩下的用来训练\n",
    "    trainMat = []\n",
    "    trainClasses = []\n",
    "    for docIndex in trainingSet:\n",
    "        #生成文档矩阵\n",
    "        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    #训练算法，计算概率\n",
    "    p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses))\n",
    "    errorCount = 0\n",
    "    #测试算法\n",
    "    for docIndex in testSet:\n",
    "        wordVector = setOfWords2Vec(vocabList, docList[docIndex])\n",
    "        #若测试输出的结果与类别标签数组classList中不同，则为错误\n",
    "        if classifyNB(array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    #错误次数\n",
    "    print 'the errorCount is: ', errorCount\n",
    "    #测试次数\n",
    "    print 'the testSet length is :', len(testSet)\n",
    "    #求错误率\n",
    "    print 'the error rate is :', float(errorCount)/len(testSet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the errorCount is:  0\n",
      "the testSet length is : 10\n",
      "the error rate is : 0.0\n"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 项目案例3: 使用朴素贝叶斯分类器从个人广告中获取区域倾向\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "开发流程\n",
    "\n",
    "收集数据: 从 RSS 源收集内容，这里需要对 RSS 源构建一个接口\n",
    "准备数据: 将文本文件解析成词条向量\n",
    "分析数据: 检查词条确保解析的正确性\n",
    "训练算法: 使用我们之前简历的 trainNB0() 函数\n",
    "测试算法: 观察错误率，确保分类器可用。可以修改切分程序，以降低错误率，提高分类结果\n",
    "使用算法: 构建一个完整的程序，封装所有内容。给定两个 RSS 源，改程序会显示最常用的公共词\n",
    "'''\n",
    "\n",
    "\n",
    "#RSS源分类器及高频词去除函数\n",
    "\n",
    "def calcMostFreq(vocabList,fullText):\n",
    "    import operator\n",
    "    freqDict={}\n",
    "    for token in vocabList:  #遍历词汇表中的每个词\n",
    "        freqDict[token]=fullText.count(token)  #统计每个词在文本中出现的次数\n",
    "    sortedFreq=sorted(freqDict.iteritems(),key=operator.itemgetter(1),reverse=True)  #根据每个词出现的次数从高到底对字典进行排序\n",
    "    return sortedFreq[:30]   #返回出现次数最高的30个单词\n",
    "\n",
    "def localWords(feed1,feed0):\n",
    "    import feedparser\n",
    "    docList=[];classList=[];fullText=[]\n",
    "    minLen=min(len(feed1['entries']),len(feed0['entries']))\n",
    "    for i in range(minLen):\n",
    "        wordList=textParse(feed1['entries'][i]['summary'])   #每次访问一条RSS源\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList=textParse(feed0['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList=createVocabList(docList)\n",
    "    top30Words=calcMostFreq(vocabList,fullText)\n",
    "    for pairW in top30Words:\n",
    "        if pairW[0] in vocabList:vocabList.remove(pairW[0])    #去掉出现次数最高的那些词\n",
    "    trainingSet=range(2*minLen);testSet=[]\n",
    "    for i in range(20):\n",
    "        randIndex=int(random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])\n",
    "    trainMat=[];trainClasses=[]\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList,docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam=trainNBO(array(trainMat),array(trainClasses))\n",
    "    errorCount=0\n",
    "    for docIndex in testSet:\n",
    "        wordVector=bagOfWords2VecMN(vocabList,docList[docIndex])\n",
    "        if classifyNB(array(wordVector),p0V,p1V,pSpam)!=classList[docIndex]:\n",
    "            errorCount+=1\n",
    "    print 'the error rate is:',float(errorCount)/len(testSet)\n",
    "    return vocabList,p0V,p1V"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
