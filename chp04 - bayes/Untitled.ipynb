{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优点：在数据较少的情况下仍然有效，可以处理多类别问题 \n",
    "### 缺点：对于输入数据的准备方式较为敏感。\n",
    "### 适用数据类型：标称型数据\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  朴素贝叶斯一般过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.收集数据：可以使用任何方法\n",
    "#### 2.准备数据：需要数值型或者布尔型数据\n",
    "#### 3.有大量特征时，绘制特征作用不大，此时使用直方图效果更好\n",
    "#### 4.训练算法：计算不同独立特征的条件概率\n",
    "#### 5.测试算法：计算错误率\n",
    "#### 6.使用算法：一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  一 准备数据：从文本中构建词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建实验样本 该函数返回的第一个变量是进行词条切分后的文档集合\n",
    "#返回的第二个变量是一个类别标签的集合。这里有两类，侮辱性和非侮辱性\n",
    "def loadDataSet():\n",
    "    postingList = [['my','dog','has','flea','problem','help','please'],\\\n",
    "                   ['maybe','not','take','him','to','dog','park','stupid'],\\\n",
    "                   ['my','dalmation','is','so','cute','I','love','him'],\\\n",
    "                   ['stop','posting','stupid','worthless','garbage'],\\\n",
    "                   ['mr','licks','ate','my','steak','how','to','stop','him'],\\\n",
    "                   ['quit','buying','worthless','dog','food','stupid']]\n",
    "    classVec = [0,1,0,1,0,1] #1代表侮辱性文字，0代表正常言论\n",
    "    return postingList,classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建一个包含在所有文档中出现的不重复词的列表\n",
    "def createVocabList(dataSet):\n",
    "    #set() 函数创建一个无序不重复元素集，可进行关系测试，删除重复数据，还可以计算交集、差集、并集等\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        #两个set集合求并集\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    #生成不重复的词汇列表\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#函数的输人参数为词汇表及某个文档 ，输出的是文档向量，向量的每一元素为1或0 ，分别表示词汇表中的单词在输人文档中是否出现\n",
    "def setOfWords2Vec(vocabList,inputSet):\n",
    "    #列表对 + 和 * 的操作符与字符串相似。+ 号用于组合列表，* 号用于重复列表。\n",
    "    returnVec = [0]*len(vocabList)#初始化一个和词汇表相同大小的向量;\n",
    "    for word in inputSet:#遍历文档中的单词\n",
    "        if word in vocabList:#如果出现了词汇表中的单词，则将输出的文档向量（returnVec）中的对应值设为1\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:print \"the word:%s is not in my Vocabulary!\" %word#否则打印不在列表中\n",
    "    #返回文档向量\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listOPosts, listClasses = loadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myVocabList = createVocabList(listOPosts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二 训练算法：从词向量计算概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入参数是文档矩阵trainMatrix，以及每篇文档类别标签所构成的向量。\n",
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    #获取在测试文档矩阵中有几篇文档\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    #获取第一篇文档的单词长度\n",
    "    numWords = len(trainMatrix[0])\n",
    "    #类别为1的个数除以总篇数，就得到类别为1的文档在总文档数中所占的比例\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num = ones(numWords);p1Num = ones(numWords)#初始化求概率的分子变量和分母变量，\n",
    "    p0Denom = 2.0;p1Denom = 2.0  #这里防止有一个p(xn|1)为0，则最后的乘积也为0，所有将分子初始化为1，分母初始化为2。\n",
    "    #p0Num = zeros(numWords);p1Num = zeros(numWords)\n",
    "    #p0Denom = 0.0;p1Denom = 0.0\n",
    "    for i in range(numTrainDocs):#遍历每一篇训练文档\n",
    "        if trainCategory[i] == 1:#如果这篇文档的类别是1\n",
    "            # 对所有属于类别1的文档向量按位置累加，\n",
    "            #最终形成的向量其实表示了在类别1下，一共包含了多少个词汇表中的不同单词（指列，一列表示一个单词，每一列都不重复）\n",
    "            #每一列的值表示该列的单词在属于类别1的所有文档中出现了几次\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])#sum函数计算属于类别1对应文档向量的和,同时进行累加，实质是求属于类别1的单词个数之和\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]#\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    #p1Vect = p1Num / p1Denom #属于类别1的单词在属于类别1的所有单词的概率，是一个向量，每一个元素都是一个单词的概率\n",
    "    #p0Vect =p0Num/p0Denom #类别0的，同上，\n",
    "    p1Vect = log(p1Num / p1Denom)#修改为取对数防止程序下溢出或者得不到正确答案\n",
    "    p0Vect = log(p0Num/p0Denom)\n",
    "    return p0Vect,p1Vect,pAbusive #返回两个类别的概率向量与一个属于侮辱性文档的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOPosts, listClasses = loadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "myVocabList = createVocabList(listOPosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMat = []#创建一个列表\n",
    "for postinDoc in listOPosts:\n",
    "    #for循环结束后，trainMat变为一个矩阵（按矩阵来理解），该矩阵的列数与词汇表myVocabList相等，行数与listOPosts相等，\n",
    "    trainMat.append(setOfWords2Vec(myVocabList,postinDoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0V,p1V,pAb = trainNB0(trainMat, listClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用算法：\n",
    "        # 将乘法转换为加法\n",
    "        # 乘法：P(C|F1F2...Fn) = P(F1F2...Fn|C)P(C)/P(F1F2...Fn)\n",
    "        # 加法：P(F1|C)*P(F2|C)....P(Fn|C)P(C) -> log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n",
    "        # :param vec2Classify: 待测数据[0,1,1,1,1...]，即要分类的向量\n",
    "        # :param p0Vec: 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表\n",
    "        #:param p1Vec: 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表\n",
    "        #:param pClass1: 类别1，侮辱性文件的出现概率\n",
    " # 计算公式  log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n",
    " #上面的计算公式，没有除以贝叶斯准则的公式的分母，也就是 P(w)，就进行概率大小的比较了。P(w) 指的是此文档在所有的文档中出现的概率\n",
    " #（此文档就是指该待测向量，在此例中一个文档就是一条留言板留言）。在此先理解为对于类别1和类别0，p（w）值一样\n",
    " # 因为 P(w) 针对的是包含侮辱和非侮辱的全部文档，所以 P(w) 是相同的。\n",
    "# P(F1F2...Fn|C)P(C) = P(F1|C)*P(F2|C)....P(Fn|C)P(C) =log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))该等式在下面求p1、p0时，\n",
    "#从右往左来理解\n",
    "## 我的理解是：这里的 vec2Classify * p1Vec 的意思就是求vec2Classify中每个词在类别1或类别2中出现的概率，vec2Classify也是由0、1组成的向量\n",
    "def classfyNB(vec2Classify, p0Vec, p1Vec, pClass1):  \n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)  \n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1-pClass1)  \n",
    "    if p1 > p0:  \n",
    "        return 1  \n",
    "    else:  \n",
    "        return 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingNB():\n",
    "    \"\"\"\n",
    "    测试朴素贝叶斯算法\n",
    "    \"\"\"\n",
    "    # 1. 加载数据集\n",
    "    listOPosts, listClasses = loadDataSet()\n",
    "    # 2. 创建单词集合\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    # 3. 创建数据矩阵，\n",
    "    trainMat = []\n",
    "    for postinDoc in listOPosts:\n",
    "        # 返回m*len(myVocabList)的矩阵， 记录的都是0，1信息\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    # 4. 训练数据\n",
    "    p0V, p1V, pAb = trainNB0(array(trainMat), array(listClasses))\n",
    "    # 5. 测试数据\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb)\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
