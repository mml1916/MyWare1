{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优点：在数据较少的情况下仍然有效，可以处理多类别问题 \n",
    "### 缺点：对于输入数据的准备方式较为敏感。\n",
    "### 适用数据类型：标称型数据\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  朴素贝叶斯一般过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.收集数据：可以使用任何方法\n",
    "#### 2.准备数据：需要数值型或者布尔型数据\n",
    "#### 3.有大量特征时，绘制特征作用不大，此时使用直方图效果更好\n",
    "#### 4.训练算法：计算不同独立特征的条件概率\n",
    "#### 5.测试算法：计算错误率\n",
    "#### 6.使用算法：一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  一 准备数据：从文本中构建词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建实验样本 该函数返回的第一个变量是进行词条切分后的文档集合\n",
    "#返回的第二个变量是一个类别标签的集合。这里有两类，侮辱性和非侮辱性\n",
    "def loadDataSet():\n",
    "    postingList = [['my','dog','has','flea','problem','help','please'],\\\n",
    "                   ['maybe','not','take','him','to','dog','park','stupid'],\\\n",
    "                   ['my','dalmation','is','so','cute','I','love','him'],\\\n",
    "                   ['stop','posting','stupid','worthless','garbage'],\\\n",
    "                   ['mr','licks','ate','my','steak','how','to','stop','him'],\\\n",
    "                   ['quit','buying','worthless','dog','food','stupid']]\n",
    "    classVec = [0,1,0,1,0,1] #1代表侮辱性文字，0代表正常言论\n",
    "    return postingList,classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建一个包含在所有文档中出现的不重复词的列表\n",
    "def createVocabList(dataSet):\n",
    "    #set() 函数创建一个无序不重复元素集，可进行关系测试，删除重复数据，还可以计算交集、差集、并集等\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        #两个set集合求并集\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    #生成不重复的词汇列表\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#函数的输人参数为词汇表及某个文档 ，输出的是文档向量，向量的每一元素为1或0 ，分别表示词汇表中的单词在输人文档中是否出现\n",
    "def setOfWords2Vec(vocabList,inputSet):\n",
    "    #列表对 + 和 * 的操作符与字符串相似。+ 号用于组合列表，* 号用于重复列表。\n",
    "    returnVec = [0]*len(vocabList)#初始化一个和词汇表相同大小的向量;\n",
    "    for word in inputSet:#遍历文档中的单词\n",
    "        if word in vocabList:#如果出现了词汇表中的单词，则将输出的文档向量（returnVec）中的对应值设为1\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:print \"the word:%s is not in my Vocabulary!\" %word#否则打印不在列表中\n",
    "    #返回文档向量\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listOPosts, listClasses = loadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myVocabList = createVocabList(listOPosts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二 训练算法：从词向量计算概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入参数是文档矩阵trainMatrix，以及每篇文档类别标签所构成的向量。\n",
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    #获取在测试文档矩阵中有几篇文档\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    #获取第一篇文档的单词长度\n",
    "    numWords = len(trainMatrix[0])\n",
    "    #类别为1的个数除以总篇数，就得到类别为1的文档在总文档数中所占的比例\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "  #  p0Num = ones(numWords);p1Num = ones(numWords)#初始化求概率的分子变量和分母变量，\n",
    "    #p0Denom = 2.0;p1Denom = 2.0  #这里防止有一个p(xn|1)为0，则最后的乘积也为0，所有将分子初始化为1，分母初始化为2。\n",
    "    p0Num = zeros(numWords);p1Num = zeros(numWords)\n",
    "    p0Denom = 0.0;p1Denom = 0.0\n",
    "    for i in range(numTrainDocs):#对每一篇训练文档\n",
    "        if trainCategory[i] == 1:#如果这篇文档的类别是1\n",
    "            # 对所有属于类别1的文档向量按位置累加，\n",
    "            #最终形成的向量其实表示了在类别1下，一共包含了多少个词汇表中的不同单词（指列，一列表示一个单词，每一列都不重复）\n",
    "            #每一列的值表示该列的单词出现了在属于类别1的所有文档中出现了几次\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])#sum函数计算对应文档向量的和,累加文档向量的和\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]#\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    #防止太多的很小的数相乘造成下溢。对乘积取对数。\n",
    "    p1Vect =p1Num/p1Denom#change to log() 类别1\n",
    "    p0Vect =p0Num/p0Denom #change to log()#返回每个类别的条件概率，不是常数，是向量，在向量里面是和词汇表向量长度相同，每个位置代表这个单词在这个类别中的概率\n",
    "    return p0Vect,p1Vect,pAbusive #返回两个向量和一个概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOPosts, listClasses = loadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "myVocabList = createVocabList(listOPosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMat = []\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList,postinDoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's equal\n"
     ]
    }
   ],
   "source": [
    "p0V,p1V,pAb = trainNB0(trainMat, listClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
