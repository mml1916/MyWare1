{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision-Tree（决策树）\n",
    "    优点：计算复杂度不高，输出结果易于理解，对中间值得缺失不敏感，可以处理不相关特征数据\n",
    "    缺点：可能会产生过度匹配问题\n",
    "    适用数据类型：数值型和标称型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Ipynb_importer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from treePlotter.ipynb\n"
     ]
    }
   ],
   "source": [
    "from treePlotter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以上两行是为了在jupyter notebook中像普通python文件一样使用import 功能，Ipynb_importer是一个解析文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在划分数据集前后信息发生的变化称为信息增益，知道如何计算增益，我们就可以计算                                                              每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算给定数据的香农熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcShannonEnt(dataSet):\n",
    "    #计算多维数组的长度，在此算出的相当于第一维的维度（一共几行）\n",
    "    numEntries = len(dataSet)\n",
    "    #创建字典\n",
    "    labelCounts = {}\n",
    "    #每次从数组取一行\n",
    "    for featVec in dataSet:\n",
    "        #featVec[-1]表示取出来的这一行中的倒数第一个元素作为字典的键\n",
    "        currentLabel = featVec[-1]\n",
    "        #判断这个键在不在字典里，labelCounts.keys()返回字典里所有的键\n",
    "        if currentLabel not in labelCounts.keys():\n",
    "            #若不在，则增加\n",
    "            labelCounts[currentLabel] = 0\n",
    "        #修改该键的键值\n",
    "        labelCounts[currentLabel] += 1\n",
    "    shannonEnt = 0.0\n",
    "    #一次取出每个键和该键的键值，键值标书其出现的频数，并用频数计算其出现的频率\n",
    "    for key in labelCounts:\n",
    "        prob = float(labelCounts[key])/numEntries\n",
    "        #用概率计算香农熵\n",
    "        shannonEnt -= prob * log(prob,2)\n",
    "    return shannonEnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算给定数据集的香农熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建数据集 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataSet():\n",
    "    dataSet = [[1, 1, 'yes'],\n",
    "               [1, 1, 'yes'],\n",
    "               [1, 0, 'no'],\n",
    "               [0, 1, 'no'],\n",
    "               [0, 1, 'no']]\n",
    "    labels = ['no surfacing', 'flippers']\n",
    "    return dataSet, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 按照给定特征划分数据集 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataSet(dataSet, axis, value):\n",
    "    retDataSet = []\n",
    "    #依次取出数据集的每一行\n",
    "    for featVec in dataSet:\n",
    "        #比较取出的一行的第一个元素和value是否相等\n",
    "        if featVec[axis] == value:\n",
    "            #featVec[:axis]中[:axis]表示的是切片索引，指取出该行数据0到aixs-1的元素\n",
    "            #若aixs为0，则featVec[:aixs]表示[],该语句实际上创建了一个列表\n",
    "            reducedFeatVec = featVec[:axis]\n",
    "            #向列表reducedFeatVec添加元素，添加的元素为featVec中从axis+1开始到结束\n",
    "            reducedFeatVec.extend(featVec[axis+1 :])\n",
    "            #向列表retDataSet中添加元素\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 选择最好的数据划分方式 (实现选取特征，划分数据集，计算出最好的划分数据集的特征)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    #获取数据集特征的数目，不包含最后一列的类标签（只用求一行的就可以确定）\n",
    "    numFeature = len(dataSet[0]) -1\n",
    "    #计算数据集的信息熵（未划分之前）\n",
    "    baseEntropy = calcShannonEnt(dataSet)\n",
    "    #定义最优信息增益\n",
    "    bestInfoGain = 0.0\n",
    "    #定义最优特征值\n",
    "    bestFeature = -1\n",
    "    #遍历所有的特征值\n",
    "    for i in range(numFeature):\n",
    "        #抽取第i个特征值得特征值列表\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        \n",
    "        #使用python原生set（集合）数据类型，该类型与列表类似，不同之处在于集合类型中的每个值互不相同\n",
    "        #从列表中穿建集合是python中得到列表中唯一元素值的最快方法\n",
    "        uniqueVals = set(featList)\n",
    "        #定义新的信息熵\n",
    "        newEntropy = 0.0\n",
    "        #遍历当前特征值列表中的所有唯一属性值值\n",
    "        for value in uniqueVals:\n",
    "            #用第i个特征值列表中的每一个唯一属性值划分一次数据集\n",
    "            subDataSet = splitDataSet(dataSet, i, value)\n",
    "            #计算对应该属性划分的子集占数据集的比例\n",
    "            prob = len(subDataSet)/float(len(dataSet))\n",
    "            #当前子集的信息熵乘以其对应的占比，并进行累加求得第i个特征值列表信息熵\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet)\n",
    "        #计算信息增益（数据集划分前后信息发生的变化）\n",
    "        infoGain = baseEntropy - newEntropy\n",
    "        #与最优信息增益进行比较，若大于，则更新最优信息增益和最优特征值（信息增益最大的特征即是最好的选择）\n",
    "        if(infoGain > bestInfoGain):\n",
    "            bestInfoGain = infoGain\n",
    "            bestFeature = i\n",
    "    #返回最优特征值\n",
    "    return bestFeature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 与chp2的classify0部分类似，该函数使用分类名称的列表，                                                                                                                               然后创建键为classList中唯一值的数据字典，字典对象中存储了                                                                                                                                  classList中每个类标签出现的频率（也即频数），最后利用                                                                                                                                            operator操作键值排序字典，并返回出现次数最多的分类名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  majorityCnt(classList):\n",
    "    classCount = {}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys():\n",
    "            classCount[vote] = 0\n",
    "        classCount[vote] += 1\n",
    "    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedClassCount[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  创建树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels标签列表，包含了数据集中所有特征的标签\n",
    "def createTree(dataSet, labels):\n",
    "    #包含数据集的所有类标签\n",
    "    classList = [example[-1] for example in dataSet]\n",
    "    #统计在classList列表中，类标签为classList[0]有几个，若个数于classList的个数相等，则说明所有的类标签完全相同\n",
    "    #作为递归函数的第一个停止条件，直接返回该类标签\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return classList[0]\n",
    "    #递归函数的第二个停止条件是使用完了所有特征，仍然不能将数据集划分成仅包含唯一类别的分组\n",
    "    #假设原先dataSet[0] = [1, 1, 'yes']，此时len（dataSet[0]） = 3,其中有两个特征（分别是第一列和第二列），\n",
    "    #最后一列是分类标签，若两个特征都用完了，则只剩最后一列的分类标签，所以len（dataSet[0]） == 1可以判断\n",
    "    #是否用完了所有特征\n",
    "    if len(dataSet[0]) == 1:\n",
    "        #由于第二个条件无法简单地返回唯一的类标签，这里使用前面的majorityEnt函数挑选出现次数最多的类别作为返回值\n",
    "        return majorityCnt(classList)\n",
    "    #当前数据选取的最好特征值的下标\n",
    "    bestFeat = chooseBestFeatureToSplit(dataSet)\n",
    "    #存储最好特征值下标的对应元素\n",
    "    bestFeatLabel = labels[bestFeat]\n",
    "    #存储了树的所有信息\n",
    "    myTree = {bestFeatLabel:{}}\n",
    "    #删除对应列表下标的元素\n",
    "    del(labels[bestFeat])\n",
    "    #遍历当前选择特征包含的所有属性值（例如当前特征为no surfacing，其属性值有yes和no）\n",
    "    featValues = [example[bestFeat] for example in dataSet]\n",
    "    #从列表中创建集合使得列表中的元素值都是唯一值\n",
    "    uniqueVals = set(featValues)\n",
    "    #例如根据当前特征值no surfacing 来选择，可以获得5个属性值（其中三个为yes，两个为no），yes为1，no为0,即[1, 1, 1, 0, 0]\n",
    "    #利用set函数将重复的值去掉，所以uniqueVals =  [1,0]\n",
    "    #遍历uniqueVals\n",
    "    for value in uniqueVals:\n",
    "        #由于在前面labels中已删去了当前用于划分的最好特征值no surfacing，因此此时labels[:]中就只剩下Flippers\n",
    "        subLabels = labels[:]\n",
    "        #根据当前的最好特征值no surfacing划分的数据集分为yes（3个）和no（2个），分别对归为yes和no的数据集再次递归调用createTree()函数\n",
    "        #直到其不能再划分，得到的返回值将被插入到字典myTree\n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet,bestFeat, value), subLabels)\n",
    "    return myTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  决策树分类函数 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在执行数据分类时，需要决策树以及用于构造树的标签向量。然后，程序比较测试数据（testVec参数表示划分数据集的第一和第二特征的属性值）与决策树上的数值，递归执行该过程直到进人叶子节点；最后将测试数据定义为叶子节点所属的类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(inputTree,featLabels,testVec):  \n",
    "    # 得到树中的第一个特征  \n",
    "    firstStr = inputTree.keys()[0]  \n",
    "    # 得到第一个对应的值  \n",
    "    secondDict = inputTree[firstStr]  \n",
    "    # 得到树中第一个特征对应的索引  \n",
    "    featIndex = featLabels.index(firstStr)  \n",
    "    # 遍历secondDict字典的键\n",
    "    for key in secondDict.keys():  \n",
    "        # 如果在secondDict[key]中找到testVec[featIndex] ，testVec参数表示划分数据集的第一和第二特征的属性值 \n",
    "        if testVec[featIndex] == key:  \n",
    "            # 判断secondDict[key]是否为字典  \n",
    "            if type(secondDict[key]).__name__ == 'dict':  \n",
    "                # 若为字典，递归继续 \n",
    "                classLabel = classify(secondDict[key], featLabels, testVec)  \n",
    "            else:  \n",
    "                # 若secondDict[key]为标签值即叶子节点，则将secondDict[key]赋给classLabel  \n",
    "                classLabel = secondDict[key]  \n",
    "    # 返回类标签  \n",
    "    return classLabel  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用pick模块存储决策树 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 决策树的序列化  \n",
    "def storeTree(inputTree,filename):  \n",
    "    # 导入pyton模块  \n",
    "    import pickle  \n",
    "    # 以写的方式打开文件  \n",
    "    fw = open(filename,'w')  \n",
    "    # 决策树序列化  \n",
    "    pickle.dump(inputTree,fw)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取序列化的树          \n",
    "def grabTree(filename):  \n",
    "    import pickle  \n",
    "    fr = open(filename)  \n",
    "    # 返回读到的树  \n",
    "    return pickle.load(fr)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用决策树预测隐形眼镜类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#收集数据，打开存放数据集的文件\n",
    "fr = open('lenses.txt')\n",
    "#准备数据集，读取文件，一次读一行存入inst ，strip()移除字符串头尾指定的字符（默认为空格）\n",
    "#split()通过指定分隔符对字符串进行切片，如果参数num 有指定值，则仅分隔 num 个子字符串\n",
    "#返回分割后的字符串列表\n",
    "lenses = [inst.strip().split('\\t') for inst in fr.readlines()]\n",
    "#标签列表包含了数据集中所有特征的标签\n",
    "lensesLabels = ['age','prescipt','astigmatic','tearRate']\n",
    "#创建树\n",
    "lensesTree = createTree(lenses, lensesLabels)\n",
    "#绘制树\n",
    "createPlot(lensesTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
